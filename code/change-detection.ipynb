{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14013488,"sourceType":"datasetVersion","datasetId":8927407}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:14:11.029129Z","iopub.execute_input":"2026-01-13T14:14:11.029386Z","iopub.status.idle":"2026-01-13T14:14:11.036449Z","shell.execute_reply.started":"2026-01-13T14:14:11.029364Z","shell.execute_reply":"2026-01-13T14:14:11.035640Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q timm==1.0.22\n# because there are no dinov3 models in the default version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:14:11.321999Z","iopub.execute_input":"2026-01-13T14:14:11.322534Z","iopub.status.idle":"2026-01-13T14:15:26.081519Z","shell.execute_reply.started":"2026-01-13T14:14:11.322511Z","shell.execute_reply":"2026-01-13T14:15:26.080443Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import timm\nprint([m for m in timm.list_models() if \"dinov3\" in m])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:15:26.083154Z","iopub.execute_input":"2026-01-13T14:15:26.083469Z","iopub.status.idle":"2026-01-13T14:15:36.105371Z","shell.execute_reply.started":"2026-01-13T14:15:26.083434Z","shell.execute_reply":"2026-01-13T14:15:36.104521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, random\nfrom pathlib import Path\n\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:15:36.106168Z","iopub.execute_input":"2026-01-13T14:15:36.106394Z","iopub.status.idle":"2026-01-13T14:15:36.112531Z","shell.execute_reply.started":"2026-01-13T14:15:36.106377Z","shell.execute_reply":"2026-01-13T14:15:36.111832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CFG:\n    root = Path(\"/kaggle/input/oscd-for-change-detection/OSCD/\")\n    out_dir = Path(\"/kaggle/working\")\n    out_dir.mkdir(parents=True, exist_ok=True)\n    pred_dir = Path(\"/kaggle/working/predictions\")\n    pred_dir.mkdir(parents=True, exist_ok=True)\n\n    train_cities = [\n        \"aguasclaras\",\"bercy\",\"bordeaux\",\"nantes\",\"paris\",\"rennes\",\"saclay_e\",\n        \"abudhabi\",\"cupertino\", \"mumbai\", \"hongkong\", \"pisa\"\n    ]\n    validation_cities = [ \n        \"beihai\", \"beirut\"\n    ]\n    test_cities = [\n        \"brasilia\",\"montpellier\",\"norcia\",\"rio\",\"saclay_w\",\"valencia\",\"dubai\",\n        \"lasvegas\",\"milano\",\"chongqing\"\n    ]\n\n    patch_size = 256\n    stride = 128\n\n    epochs = 100\n    batch_size = 4\n    freeze_backbone_epochs = 20\n    unfreeze_blocks = 2      # unfreeze only last 2 ViT blocks\n    lr_backbone = 5e-7      \n    lr_decoder  = 3e-5       \n\n\n    num_workers = 4\n    seed = 42\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    vit_name = \"vit_large_patch16_dinov3.sat493m\"\n    dinov3_ckpt = \"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:53:18.424019Z","iopub.execute_input":"2026-01-13T14:53:18.424673Z","iopub.status.idle":"2026-01-13T14:53:18.431077Z","shell.execute_reply.started":"2026-01-13T14:53:18.424633Z","shell.execute_reply":"2026-01-13T14:53:18.430373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(CFG.seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:53:19.475882Z","iopub.execute_input":"2026-01-13T14:53:19.476149Z","iopub.status.idle":"2026-01-13T14:53:19.482248Z","shell.execute_reply.started":"2026-01-13T14:53:19.476132Z","shell.execute_reply":"2026-01-13T14:53:19.481495Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport random\nimport cv2\n\ndef random_block_mask(img, num_blocks=2, max_frac=0.15):\n    H, W, C = img.shape\n    out = img.copy()\n\n    for _ in range(num_blocks):\n        bh = int(random.uniform(0.05, max_frac) * H)\n        bw = int(random.uniform(0.05, max_frac) * W)\n\n        y = random.randint(0, H - bh)\n        x = random.randint(0, W - bw)\n\n        out[y:y+bh, x:x+bw] = 0\n\n    return out\n\ndef paired_augment(pre, post, mask, scale_range=(0.8, 1.2), crop_size=256, blur_prob=0.3, jitter_prob=0.3):\n    H, W, C = pre.shape\n\n    # 1. Horizontal flip\n    if random.random() < 0.5:\n        pre = np.flip(pre, axis=1)\n        post = np.flip(post, axis=1)\n        mask = np.flip(mask, axis=1)\n\n    # 2. Vertical flip\n    if random.random() < 0.5:\n        pre = np.flip(pre, axis=0)\n        post = np.flip(post, axis=0)\n        mask = np.flip(mask, axis=0)\n\n    # 3. Rotation (0°, 90°, 180°, 270°)\n    k = random.randint(0, 3)\n    if k > 0:\n        pre = np.rot90(pre, k)\n        post = np.rot90(post, k)\n        mask = np.rot90(mask, k)\n\n    # 4. Random RESCALE (scale ∈ [0.8, 1.2])\n    scale = random.uniform(scale_range[0], scale_range[1])\n    new_h = int(H * scale)\n    new_w = int(W * scale)\n\n    pre_r = cv2.resize(pre, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n    post_r = cv2.resize(post, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n    mask_r = cv2.resize(mask, (new_w, new_h), interpolation=cv2.INTER_NEAREST)\n\n    # 5. Random CROP back to crop_size\n    if new_h > crop_size and new_w > crop_size:\n        top = random.randint(0, new_h - crop_size)\n        left = random.randint(0, new_w - crop_size)\n\n        pre_r = pre_r[top:top+crop_size, left:left+crop_size]\n        post_r = post_r[top:top+crop_size, left:left+crop_size]\n        mask_r = mask_r[top:top+crop_size, left:left+crop_size]\n    else:\n        # If scaled too small → resize back (rare)\n        pre_r  = cv2.resize(pre_r,  (crop_size, crop_size))\n        post_r = cv2.resize(post_r, (crop_size, crop_size))\n        mask_r = cv2.resize(mask_r, (crop_size, crop_size), interpolation=cv2.INTER_NEAREST)\n\n    # 6. Gaussian BLUR\n    if random.random() < blur_prob:\n        k = random.choice([3,5])\n        pre_r = cv2.GaussianBlur(pre_r,  (k,k), 0)\n        post_r = cv2.GaussianBlur(post_r, (k,k), 0)\n        # mask never blurred\n\n    # 7. COLOR JITTER (brightness, contrast, saturation)\n    if random.random() < jitter_prob:\n        # brightness\n        b = random.uniform(0.8, 1.2)\n        pre_r  = np.clip(pre_r  * b, 0, 255)\n        post_r = np.clip(post_r * b, 0, 255)\n\n        # contrast\n        c = random.uniform(0.8, 1.2)\n        pre_r  = np.clip((pre_r  - 128) * c + 128, 0, 255)\n        post_r = np.clip((post_r - 128) * c + 128, 0, 255)\n\n        # saturation (convert to HSV)\n        sat = random.uniform(0.8, 1.2)\n        pre_hsv  = cv2.cvtColor(pre_r.astype(np.uint8),  cv2.COLOR_RGB2HSV)\n        post_hsv = cv2.cvtColor(post_r.astype(np.uint8), cv2.COLOR_RGB2HSV)\n\n        pre_hsv[:,:,1]  = np.clip(pre_hsv[:,:,1]  * sat, 0, 255)\n        post_hsv[:,:,1] = np.clip(post_hsv[:,:,1] * sat, 0, 255)\n\n        pre_r  = cv2.cvtColor(pre_hsv,  cv2.COLOR_HSV2RGB)\n        post_r = cv2.cvtColor(post_hsv, cv2.COLOR_HSV2RGB)\n\n    # 8. RANDOM OCCLUSION MASKING (same for pre & post)\n    if random.random() < 0.4:\n        Hc, Wc, _ = pre_r.shape\n        bh = int(random.uniform(0.05, 0.15) * Hc)\n        bw = int(random.uniform(0.05, 0.15) * Wc)\n    \n        y = random.randint(0, Hc - bh)\n        x = random.randint(0, Wc - bw)\n    \n        pre_r[y:y+bh, x:x+bw] = 0\n        post_r[y:y+bh, x:x+bw] = 0\n\n    return pre_r.copy(), post_r.copy(), mask_r.copy()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:53:20.339347Z","iopub.execute_input":"2026-01-13T14:53:20.339950Z","iopub.status.idle":"2026-01-13T14:53:20.355314Z","shell.execute_reply.started":"2026-01-13T14:53:20.339927Z","shell.execute_reply":"2026-01-13T14:53:20.354521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Normalization\nIMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32).view(3,1,1)\nIMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32).view(3,1,1)\n\nclass OSCDDataset(Dataset):\n    def __init__(self, root: Path, cities, patch_size=256, stride=128, augment=False):\n        self.root = Path(root)\n        self.cities = cities\n        self.patch_size = patch_size\n        self.stride = stride\n        self.augment = augment\n\n        self.samples = []\n        for city in cities:\n            cdir = self.root / city\n            pre = cdir/\"img1.png\"\n            post = cdir/\"img2.png\"\n            mask = cdir/\"cm.png\"\n            if pre.exists() and post.exists() and mask.exists():\n                self.samples.append((pre, post, mask))\n            else:\n                print(f\"[WARN] Missing image in {city}\")\n\n        assert len(self.samples) > 0\n\n        self.index = []\n        for i, (pre_p, _, _) in enumerate(self.samples):\n            pre_img = Image.open(pre_p).convert(\"RGB\")\n            W, H = pre_img.size\n            for y in range(0, H - patch_size + 1, stride):\n                for x in range(0, W - patch_size + 1, stride):\n                    self.index.append((i, x, y))\n\n    def __len__(self):\n        return len(self.index)\n\n    def __getitem__(self, idx):\n        i, x, y = self.index[idx]\n        pre_p, post_p, mask_p = self.samples[i]\n        ps = self.patch_size\n\n        pre = np.array(Image.open(pre_p).convert(\"RGB\"))\n        post = np.array(Image.open(post_p).convert(\"RGB\"))\n        mask = np.array(Image.open(mask_p).convert(\"RGB\"))\n\n        pre = pre[y:y+ps, x:x+ps]\n        post = post[y:y+ps, x:x+ps]\n        mask = mask[y:y+ps, x:x+ps]\n\n        mask = (mask[...,0] > 127).astype(np.uint8)\n\n        if self.augment:\n            pre, post, mask = paired_augment(pre, post, mask)\n\n        # tensor\n        pre = torch.from_numpy(pre).permute(2,0,1).float()/255.\n        post = torch.from_numpy(post).permute(2,0,1).float()/255.\n\n        # normalize\n        pre = (pre - IMAGENET_MEAN) / IMAGENET_STD\n        post = (post - IMAGENET_MEAN) / IMAGENET_STD\n\n        mask = torch.from_numpy(mask).long()\n\n        return pre, post, mask\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:53:21.927064Z","iopub.execute_input":"2026-01-13T14:53:21.927790Z","iopub.status.idle":"2026-01-13T14:53:21.939107Z","shell.execute_reply.started":"2026-01-13T14:53:21.927764Z","shell.execute_reply":"2026-01-13T14:53:21.938275Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Only last layer of features are extracted using this backbone\n# class ViTBackboneFeatures(nn.Module):\n#     def __init__(self, vit_name=\"vit_large_patch16_dinov3.sat493m\"):\n#         super().__init__()\n#         self.backbone = timm.create_model(\n#             vit_name,\n#             pretrained=True,\n#             features_only=True,\n#         )\n#         self.out_channels = self.backbone.feature_info.channels()[-1]\n\n#     def forward(self, x):\n#         feats = self.backbone(x)\n#         f = feats[-1]\n#         return f\n\n# Mid and Last layers of features are extracted using this backbone\nclass ViTBackboneMultiScale(nn.Module):\n    def __init__(self, vit_name, out_indices=(4, 8, 12)):\n        super().__init__()\n        self.backbone = timm.create_model(\n            vit_name,\n            pretrained=True,\n            features_only=True,\n            out_indices=out_indices\n        )\n        self.channels = self.backbone.feature_info.channels()\n\n    def forward(self, x):\n        return self.backbone(x)  # list of [B,C,h,w]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:53:23.328370Z","iopub.execute_input":"2026-01-13T14:53:23.329032Z","iopub.status.idle":"2026-01-13T14:53:23.333709Z","shell.execute_reply.started":"2026-01-13T14:53:23.329009Z","shell.execute_reply":"2026-01-13T14:53:23.333071Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fusion from ChangeFormer\nclass Fusion(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.proj = nn.Conv2d(dim*3, dim, 1)\n\n    def forward(self, f_pre, f_post):\n        diff = torch.abs(f_pre - f_post)\n        x = torch.cat([f_pre, f_post, diff], dim=1)\n        return self.proj(x)\n\n# Fusion with normalization and ReLU\nclass NormFusion(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.proj = nn.Sequential(\n            nn.Conv2d(dim * 3, dim, kernel_size=1, bias=False),\n            nn.BatchNorm2d(dim),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, f_pre, f_post):\n        diff = torch.abs(f_pre - f_post)\n        fused = torch.cat([f_pre, f_post, diff], dim=1)\n        return self.proj(fused)\n\n# Fusion at mid and last layers\nclass MultiScaleFusion(nn.Module):\n    def __init__(self, in_dim, out_dim=256):\n        super().__init__()\n        self.proj = nn.Sequential(\n            nn.Conv2d(in_dim * 3, out_dim, 1, bias=False),\n            nn.BatchNorm2d(out_dim),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, f_pre, f_post):\n        diff = torch.abs(f_pre - f_post)\n        x = torch.cat([f_pre, f_post, diff], dim=1)\n        return self.proj(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:53:24.988410Z","iopub.execute_input":"2026-01-13T14:53:24.988775Z","iopub.status.idle":"2026-01-13T14:53:24.996293Z","shell.execute_reply.started":"2026-01-13T14:53:24.988754Z","shell.execute_reply":"2026-01-13T14:53:24.995478Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# FPN Decoder for multiscale feature extraction\nclass FPNDecoder(nn.Module):\n    def __init__(self, in_channels, fpn_dim=256):\n        super().__init__()\n        self.lateral = nn.ModuleList([nn.Conv2d(c, fpn_dim, 1) for c in in_channels])\n\n        self.head = nn.Sequential(\n            nn.Conv2d(fpn_dim, fpn_dim, 3, padding=1, bias=False),\n            nn.BatchNorm2d(fpn_dim),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(fpn_dim, 1, 1)\n        )\n\n    def forward(self, feats):\n        # feats ordered low->high resolution as returned by timm features_only\n        x = self.lateral[-1](feats[-1])\n        for i in reversed(range(len(feats) - 1)):\n            x = F.interpolate(x, size=feats[i].shape[-2:], mode=\"bilinear\", align_corners=False)\n            x = x + self.lateral[i](feats[i])\n        return self.head(x)  # (B,1,h,w)\n\n# Decoder for multiscale feature extraction\nclass Decoder(nn.Module):\n    def __init__(self, n_feats=3, feat_dim=256, hidden=256, n_blocks=3):\n        super().__init__()\n        in_ch = n_feats * feat_dim\n\n        self.mix = nn.Sequential(\n            nn.Conv2d(in_ch, hidden, 1, bias=False),\n            nn.BatchNorm2d(hidden),\n            nn.ReLU(inplace=True),\n        )\n\n        blocks = []\n        for _ in range(n_blocks):\n            blocks += [\n                nn.Conv2d(hidden, hidden, 3, padding=1, bias=False),\n                nn.BatchNorm2d(hidden),\n                nn.ReLU(inplace=True),\n            ]\n        self.refine = nn.Sequential(*blocks)\n        self.head = nn.Conv2d(hidden, 1, 1)\n\n    def forward(self, feats):  # feats: list of [B,256,16,16]\n        x = torch.cat(feats, dim=1)   # [B, 3*256, 16, 16]\n        x = self.mix(x)\n        x = self.refine(x)\n        return self.head(x)           # [B,1,16,16]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:53:27.401830Z","iopub.execute_input":"2026-01-13T14:53:27.402095Z","iopub.status.idle":"2026-01-13T14:53:27.410865Z","shell.execute_reply.started":"2026-01-13T14:53:27.402079Z","shell.execute_reply":"2026-01-13T14:53:27.410041Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PositionEncodingSine(nn.Module):\n    def __init__(self, num_pos_feats=128, temperature=10000):\n        super().__init__()\n        self.num_pos_feats = num_pos_feats\n        self.temperature = temperature\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        mask = torch.zeros(B, H, W, device=x.device, dtype=torch.bool)\n\n        y_embed = (~mask).cumsum(1, dtype=torch.float32)\n        x_embed = (~mask).cumsum(2, dtype=torch.float32)\n\n        eps = 1e-6\n        y_embed = y_embed / (y_embed[:, -1:, :] + eps)\n        x_embed = x_embed / (x_embed[:, :, -1:] + eps)\n\n        dim_t = torch.arange(self.num_pos_feats, device=x.device)\n        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n\n        pos_x = x_embed[:, :, :, None] / dim_t\n        pos_y = y_embed[:, :, :, None] / dim_t\n\n        pos_x = torch.stack((pos_x[..., 0::2].sin(), pos_x[..., 1::2].cos()), dim=4).flatten(3)\n        pos_y = torch.stack((pos_y[..., 0::2].sin(), pos_y[..., 1::2].cos()), dim=4).flatten(3)\n\n        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n        return pos\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:53:27.670765Z","iopub.execute_input":"2026-01-13T14:53:27.671327Z","iopub.status.idle":"2026-01-13T14:53:27.678310Z","shell.execute_reply.started":"2026-01-13T14:53:27.671305Z","shell.execute_reply":"2026-01-13T14:53:27.677556Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PixelDecoder(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.lateral4 = nn.Conv2d(dim, dim, 1)\n        self.lateral3 = nn.Conv2d(dim, dim, 1)\n        self.lateral2 = nn.Conv2d(dim, dim, 1)\n\n        self.out4 = nn.Conv2d(dim, dim, 3, padding=1)\n        self.out3 = nn.Conv2d(dim, dim, 3, padding=1)\n        self.out2 = nn.Conv2d(dim, dim, 3, padding=1)\n\n    def forward(self, x):\n        p4 = self.lateral4(x)\n        p3 = F.interpolate(p4, scale_factor=2, mode=\"bilinear\", align_corners=False)\n        p3 = self.out3(p3)\n\n        p2 = F.interpolate(p3, scale_factor=2, mode=\"bilinear\", align_corners=False)\n        p2 = self.out2(p2)\n\n        return [p2, p3, p4]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:53:27.831774Z","iopub.execute_input":"2026-01-13T14:53:27.832383Z","iopub.status.idle":"2026-01-13T14:53:27.838125Z","shell.execute_reply.started":"2026-01-13T14:53:27.832359Z","shell.execute_reply":"2026-01-13T14:53:27.837325Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Mask2FormerLayer(nn.Module):\n    def __init__(self, dim, nheads=8):\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(dim, nheads, batch_first=True)\n        self.cross_attn = nn.MultiheadAttention(dim, nheads, batch_first=True)\n\n        self.linear1 = nn.Linear(dim, dim * 4)\n        self.linear2 = nn.Linear(dim * 4, dim)\n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n        self.norm3 = nn.LayerNorm(dim)\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, queries, key_value, mask):\n        # self attention\n        q = self.norm1(queries)\n        q2, _ = self.self_attn(q, q, q)\n        q = queries + self.dropout(q2)\n\n        # masked cross attention\n        q_norm = self.norm2(q)\n\n        # flatten pixel features\n        B, C, H, W = key_value.shape\n        kv = key_value.flatten(2).transpose(1, 2)  # (B, HW, C)\n\n        # mask → attention bias\n        if mask is not None:\n            m = mask.flatten(2).permute(0, 2, 1)  # (B, HW, 1)\n            attn_mask = (m < 0.5).repeat(1, 1, q.size(1))  # False = allowed\n        else:\n            attn_mask = None\n\n        q2, _ = self.cross_attn(q_norm, kv, kv,\n                                attn_mask=None)\n        q = q + self.dropout(q2)\n\n        # FFN\n        q_norm = self.norm3(q)\n        f = self.linear2(F.relu(self.linear1(q_norm)))\n        q = q + self.dropout(f)\n\n        return q\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:53:29.798629Z","iopub.execute_input":"2026-01-13T14:53:29.799232Z","iopub.status.idle":"2026-01-13T14:53:29.806678Z","shell.execute_reply.started":"2026-01-13T14:53:29.799207Z","shell.execute_reply":"2026-01-13T14:53:29.805794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MaskHead(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.feat_proj = nn.Conv2d(dim, dim, 1)\n        self.query_proj = nn.Linear(dim, dim)\n\n    def forward(self, pixel_features, queries):\n        B, C, H, W = pixel_features.shape\n        feat = self.feat_proj(pixel_features)\n        feat_flat = feat.view(B, C, H * W)\n\n        q = self.query_proj(queries)\n\n        logits = torch.einsum(\"bnd,bdp->bnp\", q, feat_flat)\n\n        masks = logits.view(B, -1, H, W)\n        return masks\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:53:29.978227Z","iopub.execute_input":"2026-01-13T14:53:29.978521Z","iopub.status.idle":"2026-01-13T14:53:29.983799Z","shell.execute_reply.started":"2026-01-13T14:53:29.978498Z","shell.execute_reply":"2026-01-13T14:53:29.982911Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Mask2FormerDecoder(nn.Module):\n    def __init__(self, dim, num_queries=100, num_layers=4):\n        super().__init__()\n        self.dim = dim\n        self.num_queries = num_queries\n        self.num_layers = num_layers\n\n        self.query_embed = nn.Parameter(torch.randn(num_queries, dim))\n\n        self.pixel_decoder = PixelDecoder(dim)\n        self.layers = nn.ModuleList([Mask2FormerLayer(dim) for _ in range(num_layers)])\n        self.mask_head = MaskHead(dim)\n\n    def forward(self, fused_features):\n        B = fused_features.size(0)\n\n        p2, p3, p4 = self.pixel_decoder(fused_features)\n        pixel_features = p2\n        Hf, Wf = pixel_features.shape[-2:]\n\n        queries = self.query_embed.unsqueeze(0).repeat(B, 1, 1)\n\n        mask_for_next = None\n\n        for layer in self.layers:\n            queries = layer(queries, pixel_features, mask_for_next)\n            masks_q = self.mask_head(pixel_features, queries)\n            mask_for_next = masks_q.sigmoid().mean(1, keepdim=True)\n\n        # final mask = average of query masks\n        final_mask = masks_q.mean(1, keepdim=True)\n        # upsample to patch resolution (assuming 4x)\n        final_mask = F.interpolate(\n            final_mask, scale_factor=4, mode=\"bilinear\", align_corners=False\n        )\n\n        return final_mask\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:53:30.891747Z","iopub.execute_input":"2026-01-13T14:53:30.892362Z","iopub.status.idle":"2026-01-13T14:53:30.898679Z","shell.execute_reply.started":"2026-01-13T14:53:30.892342Z","shell.execute_reply":"2026-01-13T14:53:30.897969Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Only last layer of features are extracted using this model\n# class SiameseDINOv3_M2F(nn.Module):\n#     def __init__(self, vit_name):\n#         super().__init__()\n#         self.backbone = ViTBackboneFeatures(vit_name)\n#         self.dim = self.backbone.out_channels\n#         self.fusion = NormFusion(self.dim)\n#         self.decoder = Mask2FormerDecoder(self.dim)\n\n#     def forward(self, pre, post):\n#         f_pre = self.backbone(pre)\n#         f_post = self.backbone(post)\n#         fused = self.fusion(f_pre, f_post)\n#         out = self.decoder(fused)\n#         return out\n\n# Mid and Last layers of features are extracted using this model\nclass SiameseDINOv3(nn.Module):\n    def __init__(self, vit_name, out_indices=(4, 8, 12), feat_dim=256, hidden=256, n_blocks=3):\n        super().__init__()\n        self.backbone = ViTBackboneMultiScale(vit_name, out_indices=out_indices)\n        self.fusions = nn.ModuleList([MultiScaleFusion(c, feat_dim) for c in self.backbone.channels])\n        self.decoder = Decoder(\n            n_feats=len(self.backbone.channels),\n            feat_dim=feat_dim,\n            hidden=hidden,\n            n_blocks=n_blocks\n        )\n\n    def forward(self, pre, post):\n        feats_pre  = self.backbone(pre)\n        feats_post = self.backbone(post)\n\n        fused = [fus(a, b) for fus, a, b in zip(self.fusions, feats_pre, feats_post)]\n        logits = self.decoder(fused)\n\n        logits = F.interpolate(logits, size=pre.shape[-2:], mode=\"bilinear\", align_corners=False)\n        return logits  # (B,1,H,W)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:53:57.177369Z","iopub.execute_input":"2026-01-13T14:53:57.178075Z","iopub.status.idle":"2026-01-13T14:53:57.184349Z","shell.execute_reply.started":"2026-01-13T14:53:57.178047Z","shell.execute_reply":"2026-01-13T14:53:57.183484Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Binary Cross-Entropy\n# def bce_loss(logits, targets, pos_weight=3.0):\n#     targets = targets.unsqueeze(1).float()\n#     pw = torch.tensor([pos_weight], device=logits.device)\n#     return F.binary_cross_entropy_with_logits(\n#         logits, targets, pos_weight=pw\n#     )\n\n# Binary Cross-Entropy + DICE\n# def dice_loss(probs, targets, eps=1e-6):\n#     targets = targets.unsqueeze(1).float()\n#     inter = (probs * targets).sum(dim=(2,3))\n#     union = probs.sum(dim=(2,3)) + targets.sum(dim=(2,3))\n#     dice = (2*inter + eps) / (union + eps)\n#     return 1 - dice.mean()\n\n# def bce_dice_loss(logits, targets, pos_weight=3.0):\n#     pw = torch.tensor([pos_weight], device=logits.device)\n#     bce = F.binary_cross_entropy_with_logits(\n#         logits, targets.unsqueeze(1).float(), pos_weight=pw\n#     )\n#     probs = torch.sigmoid(logits)\n#     d = dice_loss(probs, targets)\n#     return bce + d\n\n# Focal loss\ndef focal_loss_with_logits(logits, targets, alpha=0.5, gamma=1.0):\n    targets = targets.unsqueeze(1).float()\n    bce = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\")\n    p = torch.sigmoid(logits)\n    pt = p*targets + (1-p)*(1-targets)          # prob of the true class\n    w = alpha*targets + (1-alpha)*(1-targets)   # class weight\n    loss = w * (1-pt).pow(gamma) * bce\n    return loss.mean()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:53:59.087151Z","iopub.execute_input":"2026-01-13T14:53:59.087851Z","iopub.status.idle":"2026-01-13T14:53:59.092793Z","shell.execute_reply.started":"2026-01-13T14:53:59.087829Z","shell.execute_reply":"2026-01-13T14:53:59.092009Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ds = OSCDDataset(CFG.root, CFG.train_cities, CFG.patch_size, CFG.stride, augment=True)\nval_ds = OSCDDataset(CFG.root, CFG.validation_cities, CFG.patch_size, CFG.stride, augment=False)\ntest_ds  = OSCDDataset(CFG.root, CFG.test_cities,  CFG.patch_size, CFG.stride, augment=False)\n\ntrain_loader = DataLoader(train_ds, batch_size=CFG.batch_size, shuffle=True,\n                          num_workers=CFG.num_workers, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size=CFG.batch_size, shuffle=False,\n                        num_workers=CFG.num_workers, pin_memory=True)\ntest_loader  = DataLoader(test_ds, batch_size=CFG.batch_size, shuffle=False,\n                          num_workers=CFG.num_workers, pin_memory=True)\n\nlen(train_ds), len(val_ds), len(test_ds)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:54:00.062933Z","iopub.execute_input":"2026-01-13T14:54:00.063514Z","iopub.status.idle":"2026-01-13T14:54:00.640044Z","shell.execute_reply.started":"2026-01-13T14:54:00.063494Z","shell.execute_reply":"2026-01-13T14:54:00.639398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\ndef evaluate(model, loader, device, threshold=0.5):\n    model.eval()\n\n    tp = fp = fn = tn = 0\n\n    for pre, post, mask in loader:\n        pre, post, mask = pre.to(device), post.to(device), mask.to(device).long()\n\n        logits = model(pre, post)\n        probs = torch.sigmoid(logits)\n        pred = (probs > threshold).long().squeeze(1)\n\n        tp += ((pred == 1) & (mask == 1)).sum().item()\n        fp += ((pred == 1) & (mask == 0)).sum().item()\n        fn += ((pred == 0) & (mask == 1)).sum().item()\n        tn += ((pred == 0) & (mask == 0)).sum().item()\n\n    eps = 1e-6\n\n    # Metrics\n    precision = tp / (tp + fp + eps)\n    recall    = tp / (tp + fn + eps)\n    f1        = (2 * precision * recall) / (precision + recall + eps)\n    iou       = tp / (tp + fp + fn + eps)\n\n    # Overall Accuracy (OA)\n    oa = (tp + tn) / (tp + tn + fp + fn + eps)\n\n    return iou, f1, precision, recall, oa\n@torch.no_grad()\ndef evaluate(model, loader, device, threshold=0.5):\n    model.eval()\n\n    tp = fp = fn = tn = 0\n\n    for pre, post, mask in loader:\n        pre, post, mask = pre.to(device), post.to(device), mask.to(device).long()\n\n        logits = model(pre, post)\n        probs = torch.sigmoid(logits)\n        pred = (probs > threshold).long().squeeze(1)\n\n        tp += ((pred == 1) & (mask == 1)).sum().item()\n        fp += ((pred == 1) & (mask == 0)).sum().item()\n        fn += ((pred == 0) & (mask == 1)).sum().item()\n        tn += ((pred == 0) & (mask == 0)).sum().item()\n\n    eps = 1e-6\n\n    # Metrics\n    precision = tp / (tp + fp + eps)\n    f1        = (2 * precision * (tp / (tp + fn + eps))) / \\\n                (precision + (tp / (tp + fn + eps)) + eps)\n    iou       = tp / (tp + fp + fn + eps)\n\n    # Accuracies\n    change_acc    = tp / (tp + fn + eps)   # OSCD Change accuracy\n    no_change_acc = tn / (tn + fp + eps)   # OSCD No-change accuracy\n    oa            = (tp + tn) / (tp + tn + fp + fn + eps)\n\n    return iou, f1, precision, oa, change_acc, no_change_acc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:54:02.147891Z","iopub.execute_input":"2026-01-13T14:54:02.148688Z","iopub.status.idle":"2026-01-13T14:54:02.158802Z","shell.execute_reply.started":"2026-01-13T14:54:02.148662Z","shell.execute_reply":"2026-01-13T14:54:02.158076Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = SiameseDINOv3(CFG.vit_name).to(CFG.device)\n\ndef unfreeze_last_vit_blocks(model, n_blocks=2):\n    # Freeze all ViT params, then unfreeze only last n transformer blocks + final norm.\n    vit = model.backbone.backbone.model\n    blocks = vit.blocks\n\n    # freeze all vit params\n    for p in vit.parameters():\n        p.requires_grad = False\n\n    # unfreeze last n blocks\n    for blk in blocks[-n_blocks:]:\n        for p in blk.parameters():\n            p.requires_grad = True\n\n    # unfreeze final norm\n    if hasattr(vit, \"norm\"):\n        for p in vit.norm.parameters():\n            p.requires_grad = True\n\n# initial freeze\nfor p in model.backbone.parameters():\n    p.requires_grad = False\n\noptimizer = torch.optim.AdamW([\n    {\"params\": model.fusions.parameters(),  \"lr\": CFG.lr_decoder},\n    {\"params\": model.decoder.parameters(), \"lr\": CFG.lr_decoder},\n    {\"params\": filter(lambda p: p.requires_grad, model.backbone.parameters()), \"lr\": CFG.lr_backbone},\n], weight_decay=1e-4)\n\nscheduler = torch.optim.lr_scheduler.LinearLR(\n    optimizer, start_factor=1.0, end_factor=0.0, total_iters=CFG.epochs\n)\n\nhistory = {\"loss\": [], \"iou\": [], \"f1\": [], \"prec\": [], \"oa\": [], \"chg\": [], \"no_chg\": []}\nbest_iou = 0.0\nhas_unfrozen = False\n\nfor epoch in range(CFG.epochs):\n    model.train()\n\n    # Partial unfreeze trigger (only once)\n    if (epoch >= CFG.freeze_backbone_epochs) and (not has_unfrozen):\n        print(f\"[INFO] Unfreezing last {CFG.unfreeze_blocks} ViT blocks at epoch {epoch}\")\n        unfreeze_last_vit_blocks(model, n_blocks=CFG.unfreeze_blocks)\n        has_unfrozen = True\n\n        # rebuild optimizer so newly-unfrozen backbone params are included\n        optimizer = torch.optim.AdamW([\n            {\"params\": model.fusions.parameters(),  \"lr\": CFG.lr_decoder},\n            {\"params\": model.decoder.parameters(), \"lr\": CFG.lr_decoder},\n            {\"params\": filter(lambda p: p.requires_grad, model.backbone.parameters()), \"lr\": CFG.lr_backbone},\n        ], weight_decay=1e-4)\n\n        scheduler = torch.optim.lr_scheduler.LinearLR(\n            optimizer,\n            start_factor=1.0,\n            end_factor=0.0,\n            total_iters=CFG.epochs - epoch\n        )\n\n    running = 0.0\n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CFG.epochs}\")\n\n    for pre, post, mask in pbar:\n        pre  = pre.to(CFG.device, non_blocking=True)\n        post = post.to(CFG.device, non_blocking=True)\n        mask = mask.to(CFG.device, non_blocking=True)\n\n        logits = model(pre, post)\n        loss = focal_loss_with_logits(logits, mask)\n\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n\n        running += loss.item()\n        pbar.set_postfix(loss=running / (pbar.n + 1e-6))\n\n    scheduler.step()\n\n    # ---- VALIDATION ----\n    iou, f1, precision, oa, change_acc, no_change_acc = evaluate(\n        model, val_loader, CFG.device, threshold=0.35\n    )\n\n    print(\n        f\"[VAL] IoU={iou:.4f} | F1={f1:.4f} | \"\n        f\"P={precision:.4f} | OA={oa:.4f} | \"\n        f\"ChangeAcc={change_acc:.4f} | NoChangeAcc={no_change_acc:.4f}\"\n    )\n\n    # log\n    history[\"loss\"].append(running / max(1, len(train_loader)))\n    history[\"iou\"].append(iou)\n    history[\"f1\"].append(f1)\n    history[\"prec\"].append(precision)\n    history[\"oa\"].append(oa)\n    history[\"chg\"].append(change_acc)\n    history[\"no_chg\"].append(no_change_acc)\n\n    # save best\n    if iou > best_iou:\n        best_iou = iou\n        torch.save(model.state_dict(), CFG.out_dir / \"best.pt\")\n        print(\"  saved best.pt\")\n\nprint(\"Training done. Best IoU:\", best_iou)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:55:09.470829Z","iopub.execute_input":"2026-01-13T14:55:09.471166Z","iopub.status.idle":"2026-01-13T15:14:41.141466Z","shell.execute_reply.started":"2026-01-13T14:55:09.471143Z","shell.execute_reply":"2026-01-13T15:14:41.140732Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(20, 10))\n\n# 1. Loss\nplt.subplot(2, 4, 1)\nplt.plot(history[\"loss\"], '-o')\nplt.title(\"Training Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\n\n# 2. IoU\nplt.subplot(2, 4, 2)\nplt.plot(history[\"iou\"], '-o')\nplt.title(\"Validation IoU\")\nplt.xlabel(\"Epoch\")\n\n# 3. F1\nplt.subplot(2, 4, 3)\nplt.plot(history[\"f1\"], '-o')\nplt.title(\"Validation F1\")\nplt.xlabel(\"Epoch\")\n\n# 4. Precision\nplt.subplot(2, 4, 4)\nplt.plot(history[\"prec\"], '-o')\nplt.title(\"Validation Precision\")\nplt.xlabel(\"Epoch\")\n\n# 5. Change Accuracy (OSCD)\nplt.subplot(2, 4, 5)\nplt.plot(history[\"chg\"], '-o')\nplt.title(\"Validation Change Acc.\")\nplt.xlabel(\"Epoch\")\n\n# 6. No-change Accuracy (OSCD)\nplt.subplot(2, 4, 6)\nplt.plot(history[\"no_chg\"], '-o')\nplt.title(\"Validation No-change Acc.\")\nplt.xlabel(\"Epoch\")\n\n\n# 7. Overall Accuracy\nplt.subplot(2, 4, 7)\nplt.plot(history[\"oa\"], '-o')\nplt.title(\"Validation Overall Accuracy\")\nplt.xlabel(\"Epoch\")\n\nplt.tight_layout()\nplt.savefig(\"training_metrics.png\", dpi=300)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T15:14:44.314208Z","iopub.execute_input":"2026-01-13T15:14:44.315026Z","iopub.status.idle":"2026-01-13T15:14:46.933246Z","shell.execute_reply.started":"2026-01-13T15:14:44.314992Z","shell.execute_reply":"2026-01-13T15:14:46.932460Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\n\ndef tune_threshold(model, loader, device):\n    thresholds = np.linspace(0.2, 0.6, 41)\n    best = {\"thr\": 0.5, \"f1\": -1.0}\n\n    for thr in thresholds:\n        iou, f1, prec, oa, change_acc, no_change_acc = evaluate(\n            model, loader, device, threshold=thr\n        )\n\n        if f1 > best[\"f1\"]:\n            best = {\n                \"thr\": float(thr),\n                \"iou\": float(iou),\n                \"f1\": float(f1),\n                \"prec\": float(prec),\n                \"oa\": float(oa),\n                \"change_acc\": float(change_acc),\n                \"no_change_acc\": float(no_change_acc),\n            }\n\n    return best\n\n\nmodel.load_state_dict(torch.load(CFG.out_dir/\"best.pt\", map_location=CFG.device))\nbest = tune_threshold(model, val_loader, CFG.device)\n\nprint(\n    f\"[VAL-THR] best_thr={best['thr']:.2f} | \"\n    f\"IoU={best['iou']:.4f} | F1={best['f1']:.4f} | \"\n    f\"Prec={best['prec']:.4f} | \"\n    f\"ChangeAcc={best['change_acc']:.4f} | NoChangeAcc={best['no_change_acc']:.4f} | \"\n    f\"OA={best['oa']:.4f}\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T15:14:56.736008Z","iopub.execute_input":"2026-01-13T15:14:56.736692Z","iopub.status.idle":"2026-01-13T15:18:07.962644Z","shell.execute_reply.started":"2026-01-13T15:14:56.736668Z","shell.execute_reply":"2026-01-13T15:18:07.961634Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def sliding_indices(length, patch_size, stride):\n    # If image is smaller than one patch, we will just use start at 0\n    if length <= patch_size:\n        return [0]\n\n    idxs = list(range(0, length - patch_size + 1, stride))\n    last_start = length - patch_size\n    if idxs[-1] != last_start:\n        idxs.append(last_start)\n    return idxs\n\n\n\ndef pad_to_patch(x, patch_size):\n    h, w, c = x.shape\n    pad_h = max(0, patch_size - h)\n    pad_w = max(0, patch_size - w)\n    if pad_h > 0 or pad_w > 0:\n        x = np.pad(x, ((0, pad_h), (0, pad_w), (0, 0)), mode=\"reflect\")\n    return x\n\n@torch.no_grad()\ndef tta_predict_prob(model, pp_t, qq_t):\n    preds = []\n\n    # original\n    logits = model(pp_t, qq_t)\n    preds.append(torch.sigmoid(logits))\n\n    # horizontal flip\n    logits = model(torch.flip(pp_t, dims=[3]), torch.flip(qq_t, dims=[3]))\n    preds.append(torch.flip(torch.sigmoid(logits), dims=[3]))\n\n    # vertical flip\n    logits = model(torch.flip(pp_t, dims=[2]), torch.flip(qq_t, dims=[2]))\n    preds.append(torch.flip(torch.sigmoid(logits), dims=[2]))\n\n    # hv flip (both)\n    logits = model(torch.flip(pp_t, dims=[2,3]), torch.flip(qq_t, dims=[2,3]))\n    preds.append(torch.flip(torch.sigmoid(logits), dims=[2,3]))\n\n    prob = torch.stack(preds, dim=0).mean(dim=0)\n    return prob[0, 0].cpu().numpy()\n\n@torch.no_grad()\ndef predict_city(model, city, patch_size=256, stride=128, threshold=0.5):\n    model.eval()\n\n    city_dir = CFG.root / city\n    pre  = np.array(Image.open(city_dir/\"img1.png\").convert(\"RGB\"))\n    post = np.array(Image.open(city_dir/\"img2.png\").convert(\"RGB\"))\n\n    H, W, _ = pre.shape\n    out = np.zeros((H, W), dtype=np.float32)\n    cnt = np.zeros((H, W), dtype=np.float32)\n\n    ys = sliding_indices(H, patch_size, stride)\n    xs = sliding_indices(W, patch_size, stride)\n\n    for y in ys:\n        for x in xs:\n            pp = pre[y:y+patch_size, x:x+patch_size]\n            qq = post[y:y+patch_size, x:x+patch_size]\n\n            # actual (unpadded) region size\n            h0, w0 = pp.shape[:2]\n\n            # pad to patch_size x patch_size if needed\n            pp = pad_to_patch(pp, patch_size)\n            qq = pad_to_patch(qq, patch_size)\n\n            # to tensor + normalize\n            pp_t = torch.from_numpy(pp).permute(2,0,1).float() / 255.\n            qq_t = torch.from_numpy(qq).permute(2,0,1).float() / 255.\n\n            pp_t = (pp_t - IMAGENET_MEAN) / IMAGENET_STD\n            qq_t = (qq_t - IMAGENET_MEAN) / IMAGENET_STD\n\n            pp_t = pp_t.unsqueeze(0).to(CFG.device)\n            qq_t = qq_t.unsqueeze(0).to(CFG.device)\n\n            logits = model(pp_t, qq_t)\n            prob = torch.sigmoid(logits)[0, 0].cpu().numpy()  # (patch_size, patch_size)\n            # prob = tta_predict_prob(model, pp_t, qq_t) # Make use of TTA\n\n\n            # write back ONLY the valid part (before padding)\n            out[y:y+h0, x:x+w0] += prob[:h0, :w0]\n            cnt[y:y+h0, x:x+w0] += 1.0\n\n    out /= np.maximum(cnt, 1e-6)\n    return out, (out > threshold).astype(np.uint8) * 255","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T15:18:15.525895Z","iopub.execute_input":"2026-01-13T15:18:15.526721Z","iopub.status.idle":"2026-01-13T15:18:15.550276Z","shell.execute_reply.started":"2026-01-13T15:18:15.526678Z","shell.execute_reply":"2026-01-13T15:18:15.549249Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nTest Evaluation\")\nmodel.eval()\n\nbest_thr = best[\"thr\"]\n\ntest_iou, test_f1, test_prec, test_oa, test_change_acc, test_no_change_acc = evaluate(\n    model, test_loader, CFG.device, threshold=best_thr\n)\n\nprint(\n    f\"[TEST] \"\n    f\"IoU={test_iou:.4f} | \"\n    f\"F1={test_f1:.4f} | \"\n    f\"Prec={test_prec:.4f} | \"\n    f\"ChangeAcc={test_change_acc:.4f} | \"\n    f\"NoChangeAcc={test_no_change_acc:.4f} | \"\n    f\"OA={test_oa:.4f}\"\n)\n\nprint(\"\\nSaving sample Test predictions\")\n\nfor city in CFG.test_cities:\n    prob, pred = predict_city(model, city, threshold=best_thr)\n    out_path = CFG.pred_dir / f\"{city}_pred.png\"\n    Image.fromarray(pred).save(out_path)\n    print(f\"Saved: {out_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T15:18:21.256885Z","iopub.execute_input":"2026-01-13T15:18:21.257188Z","iopub.status.idle":"2026-01-13T15:18:31.617538Z","shell.execute_reply.started":"2026-01-13T15:18:21.257158Z","shell.execute_reply":"2026-01-13T15:18:31.616697Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}