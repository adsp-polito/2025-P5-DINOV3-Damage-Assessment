{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T14:21:02.366268Z",
     "iopub.status.busy": "2026-01-03T14:21:02.365921Z",
     "iopub.status.idle": "2026-01-03T14:21:02.374964Z",
     "shell.execute_reply": "2026-01-03T14:21:02.374082Z",
     "shell.execute_reply.started": "2026-01-03T14:21:02.366235Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T14:21:02.581355Z",
     "iopub.status.busy": "2026-01-03T14:21:02.581095Z",
     "iopub.status.idle": "2026-01-03T14:22:10.789804Z",
     "shell.execute_reply": "2026-01-03T14:22:10.788918Z",
     "shell.execute_reply.started": "2026-01-03T14:21:02.581316Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q timm==1.0.22\n",
    "# because there are no dinov3 models in the default version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T14:22:47.762488Z",
     "iopub.status.busy": "2026-01-03T14:22:47.762141Z",
     "iopub.status.idle": "2026-01-03T14:22:56.724469Z",
     "shell.execute_reply": "2026-01-03T14:22:56.723866Z",
     "shell.execute_reply.started": "2026-01-03T14:22:47.762458Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vit_7b_patch16_dinov3', 'vit_base_patch16_dinov3', 'vit_base_patch16_dinov3_qkvb', 'vit_huge_plus_patch16_dinov3', 'vit_huge_plus_patch16_dinov3_qkvb', 'vit_large_patch16_dinov3', 'vit_large_patch16_dinov3_qkvb', 'vit_small_patch16_dinov3', 'vit_small_patch16_dinov3_qkvb', 'vit_small_plus_patch16_dinov3', 'vit_small_plus_patch16_dinov3_qkvb']\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "print([m for m in timm.list_models() if \"dinov3\" in m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-03T14:22:57.989934Z",
     "iopub.status.busy": "2026-01-03T14:22:57.989651Z",
     "iopub.status.idle": "2026-01-03T14:22:57.994099Z",
     "shell.execute_reply": "2026-01-03T14:22:57.993288Z",
     "shell.execute_reply.started": "2026-01-03T14:22:57.989912Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T14:22:58.325748Z",
     "iopub.status.busy": "2026-01-03T14:22:58.325478Z",
     "iopub.status.idle": "2026-01-03T14:22:58.388229Z",
     "shell.execute_reply": "2026-01-03T14:22:58.387563Z",
     "shell.execute_reply.started": "2026-01-03T14:22:58.325729Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    root = Path(\"OSCD/\")\n",
    "    out_dir = Path(\"runs/outputs\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    pred_dir = Path(\"runs/predictions\")\n",
    "    pred_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_cities = [\n",
    "        \"aguasclaras\",\"bercy\",\"bordeaux\",\"nantes\",\"paris\",\"rennes\",\"saclay_e\",\n",
    "        \"abudhabi\",\"cupertino\", \"mumbai\", \"hongkong\", \"pisa\"\n",
    "    ]\n",
    "    validation_cities = [ \n",
    "        \"beihai\", \"beirut\"\n",
    "    ]\n",
    "    test_cities = [\n",
    "        \"brasilia\",\"montpellier\",\"norcia\",\"rio\",\"saclay_w\",\"valencia\",\"dubai\",\n",
    "        \"lasvegas\",\"milano\",\"chongqing\"\n",
    "    ]\n",
    "\n",
    "    patch_size = 256\n",
    "    stride = 128\n",
    "\n",
    "    epochs = 100\n",
    "    batch_size = 4\n",
    "    freeze_backbone_epochs = 20\n",
    "    unfreeze_blocks = 2      # unfreeze only last 2 ViT blocks\n",
    "    lr_backbone = 5e-7      \n",
    "    lr_decoder  = 3e-5       \n",
    "\n",
    "\n",
    "    num_workers = 4\n",
    "    seed = 42\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    vit_name = \"vit_large_patch16_dinov3.sat493m\"\n",
    "    path_ckpt = \"runs/models/m9.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T14:22:58.614056Z",
     "iopub.status.busy": "2026-01-03T14:22:58.613809Z",
     "iopub.status.idle": "2026-01-03T14:22:58.623296Z",
     "shell.execute_reply": "2026-01-03T14:22:58.622670Z",
     "shell.execute_reply.started": "2026-01-03T14:22:58.614038Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T14:22:58.874191Z",
     "iopub.status.busy": "2026-01-03T14:22:58.874002Z",
     "iopub.status.idle": "2026-01-03T14:22:59.070532Z",
     "shell.execute_reply": "2026-01-03T14:22:59.069974Z",
     "shell.execute_reply.started": "2026-01-03T14:22:58.874176Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "def random_block_mask(img, num_blocks=2, max_frac=0.15):\n",
    "    H, W, C = img.shape\n",
    "    out = img.copy()\n",
    "\n",
    "    for _ in range(num_blocks):\n",
    "        bh = int(random.uniform(0.05, max_frac) * H)\n",
    "        bw = int(random.uniform(0.05, max_frac) * W)\n",
    "\n",
    "        y = random.randint(0, H - bh)\n",
    "        x = random.randint(0, W - bw)\n",
    "\n",
    "        out[y:y+bh, x:x+bw] = 0\n",
    "\n",
    "    return out\n",
    "\n",
    "def paired_augment(pre, post, mask, scale_range=(0.8, 1.2), crop_size=256, blur_prob=0.3, jitter_prob=0.3):\n",
    "    H, W, C = pre.shape\n",
    "\n",
    "    # 1. Horizontal flip\n",
    "    if random.random() < 0.5:\n",
    "        pre = np.flip(pre, axis=1)\n",
    "        post = np.flip(post, axis=1)\n",
    "        mask = np.flip(mask, axis=1)\n",
    "\n",
    "    # 2. Vertical flip\n",
    "    if random.random() < 0.5:\n",
    "        pre = np.flip(pre, axis=0)\n",
    "        post = np.flip(post, axis=0)\n",
    "        mask = np.flip(mask, axis=0)\n",
    "\n",
    "    # 3. Rotation (0°, 90°, 180°, 270°)\n",
    "    k = random.randint(0, 3)\n",
    "    if k > 0:\n",
    "        pre = np.rot90(pre, k)\n",
    "        post = np.rot90(post, k)\n",
    "        mask = np.rot90(mask, k)\n",
    "\n",
    "    # 4. Random RESCALE (scale ∈ [0.8, 1.2])\n",
    "    scale = random.uniform(scale_range[0], scale_range[1])\n",
    "    new_h = int(H * scale)\n",
    "    new_w = int(W * scale)\n",
    "\n",
    "    pre_r = cv2.resize(pre, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "    post_r = cv2.resize(post, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "    mask_r = cv2.resize(mask, (new_w, new_h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # 5. Random CROP back to crop_size\n",
    "    if new_h > crop_size and new_w > crop_size:\n",
    "        top = random.randint(0, new_h - crop_size)\n",
    "        left = random.randint(0, new_w - crop_size)\n",
    "\n",
    "        pre_r = pre_r[top:top+crop_size, left:left+crop_size]\n",
    "        post_r = post_r[top:top+crop_size, left:left+crop_size]\n",
    "        mask_r = mask_r[top:top+crop_size, left:left+crop_size]\n",
    "    else:\n",
    "        # If scaled too small → resize back (rare)\n",
    "        pre_r  = cv2.resize(pre_r,  (crop_size, crop_size))\n",
    "        post_r = cv2.resize(post_r, (crop_size, crop_size))\n",
    "        mask_r = cv2.resize(mask_r, (crop_size, crop_size), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # 6. Gaussian BLUR\n",
    "    if random.random() < blur_prob:\n",
    "        k = random.choice([3,5])\n",
    "        pre_r = cv2.GaussianBlur(pre_r,  (k,k), 0)\n",
    "        post_r = cv2.GaussianBlur(post_r, (k,k), 0)\n",
    "        # mask never blurred\n",
    "\n",
    "    # 7. COLOR JITTER (brightness, contrast, saturation)\n",
    "    if random.random() < jitter_prob:\n",
    "        # brightness\n",
    "        b = random.uniform(0.8, 1.2)\n",
    "        pre_r  = np.clip(pre_r  * b, 0, 255)\n",
    "        post_r = np.clip(post_r * b, 0, 255)\n",
    "\n",
    "        # contrast\n",
    "        c = random.uniform(0.8, 1.2)\n",
    "        pre_r  = np.clip((pre_r  - 128) * c + 128, 0, 255)\n",
    "        post_r = np.clip((post_r - 128) * c + 128, 0, 255)\n",
    "\n",
    "        # saturation (convert to HSV)\n",
    "        sat = random.uniform(0.8, 1.2)\n",
    "        pre_hsv  = cv2.cvtColor(pre_r.astype(np.uint8),  cv2.COLOR_RGB2HSV)\n",
    "        post_hsv = cv2.cvtColor(post_r.astype(np.uint8), cv2.COLOR_RGB2HSV)\n",
    "\n",
    "        pre_hsv[:,:,1]  = np.clip(pre_hsv[:,:,1]  * sat, 0, 255)\n",
    "        post_hsv[:,:,1] = np.clip(post_hsv[:,:,1] * sat, 0, 255)\n",
    "\n",
    "        pre_r  = cv2.cvtColor(pre_hsv,  cv2.COLOR_HSV2RGB)\n",
    "        post_r = cv2.cvtColor(post_hsv, cv2.COLOR_HSV2RGB)\n",
    "\n",
    "    # 8. RANDOM OCCLUSION MASKING (same for pre & post)\n",
    "    if random.random() < 0.4:\n",
    "        Hc, Wc, _ = pre_r.shape\n",
    "        bh = int(random.uniform(0.05, 0.15) * Hc)\n",
    "        bw = int(random.uniform(0.05, 0.15) * Wc)\n",
    "    \n",
    "        y = random.randint(0, Hc - bh)\n",
    "        x = random.randint(0, Wc - bw)\n",
    "    \n",
    "        pre_r[y:y+bh, x:x+bw] = 0\n",
    "        post_r[y:y+bh, x:x+bw] = 0\n",
    "\n",
    "    return pre_r.copy(), post_r.copy(), mask_r.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T14:22:59.126087Z",
     "iopub.status.busy": "2026-01-03T14:22:59.125856Z",
     "iopub.status.idle": "2026-01-03T14:22:59.138115Z",
     "shell.execute_reply": "2026-01-03T14:22:59.137521Z",
     "shell.execute_reply.started": "2026-01-03T14:22:59.126068Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Normalization\n",
    "IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32).view(3,1,1)\n",
    "IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32).view(3,1,1)\n",
    "\n",
    "class OSCDDataset(Dataset):\n",
    "    def __init__(self, root: Path, cities, patch_size=256, stride=128, augment=False):\n",
    "        self.root = Path(root)\n",
    "        self.cities = cities\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        self.augment = augment\n",
    "\n",
    "        self.samples = []\n",
    "        for city in cities:\n",
    "            cdir = self.root / city\n",
    "            pre = cdir/\"img1.png\"\n",
    "            post = cdir/\"img2.png\"\n",
    "            mask = cdir/\"cm.png\"\n",
    "            if pre.exists() and post.exists() and mask.exists():\n",
    "                self.samples.append((pre, post, mask))\n",
    "            else:\n",
    "                print(f\"[WARN] Missing image in {city}\")\n",
    "\n",
    "        assert len(self.samples) > 0\n",
    "\n",
    "        self.index = []\n",
    "        for i, (pre_p, _, _) in enumerate(self.samples):\n",
    "            pre_img = Image.open(pre_p).convert(\"RGB\")\n",
    "            W, H = pre_img.size\n",
    "            for y in range(0, H - patch_size + 1, stride):\n",
    "                for x in range(0, W - patch_size + 1, stride):\n",
    "                    self.index.append((i, x, y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i, x, y = self.index[idx]\n",
    "        pre_p, post_p, mask_p = self.samples[i]\n",
    "        ps = self.patch_size\n",
    "\n",
    "        pre = np.array(Image.open(pre_p).convert(\"RGB\"))\n",
    "        post = np.array(Image.open(post_p).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_p).convert(\"RGB\"))\n",
    "\n",
    "        pre = pre[y:y+ps, x:x+ps]\n",
    "        post = post[y:y+ps, x:x+ps]\n",
    "        mask = mask[y:y+ps, x:x+ps]\n",
    "\n",
    "        mask = (mask[...,0] > 127).astype(np.uint8)\n",
    "\n",
    "        if self.augment:\n",
    "            pre, post, mask = paired_augment(pre, post, mask)\n",
    "\n",
    "        # tensor\n",
    "        pre = torch.from_numpy(pre).permute(2,0,1).float()/255.\n",
    "        post = torch.from_numpy(post).permute(2,0,1).float()/255.\n",
    "\n",
    "        # normalize\n",
    "        pre = (pre - IMAGENET_MEAN) / IMAGENET_STD\n",
    "        post = (post - IMAGENET_MEAN) / IMAGENET_STD\n",
    "\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "\n",
    "        return pre, post, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T14:22:59.358586Z",
     "iopub.status.busy": "2026-01-03T14:22:59.358361Z",
     "iopub.status.idle": "2026-01-03T14:22:59.363390Z",
     "shell.execute_reply": "2026-01-03T14:22:59.362799Z",
     "shell.execute_reply.started": "2026-01-03T14:22:59.358570Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Only last layer of features are extracted using this backbone\n",
    "# class ViTBackboneFeatures(nn.Module):\n",
    "#     def __init__(self, vit_name=\"vit_large_patch16_dinov3.sat493m\"):\n",
    "#         super().__init__()\n",
    "#         self.backbone = timm.create_model(\n",
    "#             vit_name,\n",
    "#             pretrained=True,\n",
    "#             features_only=True,\n",
    "#         )\n",
    "#         self.out_channels = self.backbone.feature_info.channels()[-1]\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         feats = self.backbone(x)\n",
    "#         f = feats[-1]\n",
    "#         return f\n",
    "\n",
    "# Mid and Last layers of features are extracted using this backbone\n",
    "class ViTBackboneMultiScale(nn.Module):\n",
    "    def __init__(self, vit_name, out_indices=(4, 8, 12)):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            vit_name,\n",
    "            pretrained=True,\n",
    "            features_only=True,\n",
    "            out_indices=out_indices\n",
    "        )\n",
    "        self.channels = self.backbone.feature_info.channels()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)  # list of [B,C,h,w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T14:22:59.608617Z",
     "iopub.status.busy": "2026-01-03T14:22:59.608428Z",
     "iopub.status.idle": "2026-01-03T14:22:59.615139Z",
     "shell.execute_reply": "2026-01-03T14:22:59.614469Z",
     "shell.execute_reply.started": "2026-01-03T14:22:59.608603Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Fusion from ChangeFormer\n",
    "class Fusion(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(dim*3, dim, 1)\n",
    "\n",
    "    def forward(self, f_pre, f_post):\n",
    "        diff = torch.abs(f_pre - f_post)\n",
    "        x = torch.cat([f_pre, f_post, diff], dim=1)\n",
    "        return self.proj(x)\n",
    "\n",
    "# Fusion with normalization and ReLU\n",
    "class NormFusion(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Conv2d(dim * 3, dim, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, f_pre, f_post):\n",
    "        diff = torch.abs(f_pre - f_post)\n",
    "        fused = torch.cat([f_pre, f_post, diff], dim=1)\n",
    "        return self.proj(fused)\n",
    "\n",
    "# Fusion at mid and last layers\n",
    "class MultiScaleFusion(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim=256):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Conv2d(in_dim * 3, out_dim, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, f_pre, f_post):\n",
    "        diff = torch.abs(f_pre - f_post)\n",
    "        x = torch.cat([f_pre, f_post, diff], dim=1)\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T14:22:59.776778Z",
     "iopub.status.busy": "2026-01-03T14:22:59.776597Z",
     "iopub.status.idle": "2026-01-03T14:22:59.782211Z",
     "shell.execute_reply": "2026-01-03T14:22:59.781652Z",
     "shell.execute_reply.started": "2026-01-03T14:22:59.776766Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# FPN Decoder for multiscale feature extraction\n",
    "class FPNDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, fpn_dim=256):\n",
    "        super().__init__()\n",
    "        self.lateral = nn.ModuleList([nn.Conv2d(c, fpn_dim, 1) for c in in_channels])\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(fpn_dim, fpn_dim, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(fpn_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(fpn_dim, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, feats):\n",
    "        # feats ordered low->high resolution as returned by timm features_only\n",
    "        x = self.lateral[-1](feats[-1])\n",
    "        for i in reversed(range(len(feats) - 1)):\n",
    "            x = F.interpolate(x, size=feats[i].shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "            x = x + self.lateral[i](feats[i])\n",
    "        return self.head(x)  # (B,1,h,w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T14:23:06.443332Z",
     "iopub.status.busy": "2026-01-03T14:23:06.442584Z",
     "iopub.status.idle": "2026-01-03T14:23:06.450413Z",
     "shell.execute_reply": "2026-01-03T14:23:06.449656Z",
     "shell.execute_reply.started": "2026-01-03T14:23:06.443302Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionEncodingSine(nn.Module):\n",
    "    def __init__(self, num_pos_feats=128, temperature=10000):\n",
    "        super().__init__()\n",
    "        self.num_pos_feats = num_pos_feats\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        mask = torch.zeros(B, H, W, device=x.device, dtype=torch.bool)\n",
    "\n",
    "        y_embed = (~mask).cumsum(1, dtype=torch.float32)\n",
    "        x_embed = (~mask).cumsum(2, dtype=torch.float32)\n",
    "\n",
    "        eps = 1e-6\n",
    "        y_embed = y_embed / (y_embed[:, -1:, :] + eps)\n",
    "        x_embed = x_embed / (x_embed[:, :, -1:] + eps)\n",
    "\n",
    "        dim_t = torch.arange(self.num_pos_feats, device=x.device)\n",
    "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
    "\n",
    "        pos_x = x_embed[:, :, :, None] / dim_t\n",
    "        pos_y = y_embed[:, :, :, None] / dim_t\n",
    "\n",
    "        pos_x = torch.stack((pos_x[..., 0::2].sin(), pos_x[..., 1::2].cos()), dim=4).flatten(3)\n",
    "        pos_y = torch.stack((pos_y[..., 0::2].sin(), pos_y[..., 1::2].cos()), dim=4).flatten(3)\n",
    "\n",
    "        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
    "        return pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T14:23:06.613782Z",
     "iopub.status.busy": "2026-01-03T14:23:06.613543Z",
     "iopub.status.idle": "2026-01-03T14:23:06.619071Z",
     "shell.execute_reply": "2026-01-03T14:23:06.618323Z",
     "shell.execute_reply.started": "2026-01-03T14:23:06.613765Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PixelDecoder(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.lateral4 = nn.Conv2d(dim, dim, 1)\n",
    "        self.lateral3 = nn.Conv2d(dim, dim, 1)\n",
    "        self.lateral2 = nn.Conv2d(dim, dim, 1)\n",
    "\n",
    "        self.out4 = nn.Conv2d(dim, dim, 3, padding=1)\n",
    "        self.out3 = nn.Conv2d(dim, dim, 3, padding=1)\n",
    "        self.out2 = nn.Conv2d(dim, dim, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p4 = self.lateral4(x)\n",
    "        p3 = F.interpolate(p4, scale_factor=2, mode=\"bilinear\", align_corners=False)\n",
    "        p3 = self.out3(p3)\n",
    "\n",
    "        p2 = F.interpolate(p3, scale_factor=2, mode=\"bilinear\", align_corners=False)\n",
    "        p2 = self.out2(p2)\n",
    "\n",
    "        return [p2, p3, p4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T14:24:08.148851Z",
     "iopub.status.busy": "2026-01-03T14:24:08.148551Z",
     "iopub.status.idle": "2026-01-03T14:24:08.155840Z",
     "shell.execute_reply": "2026-01-03T14:24:08.155186Z",
     "shell.execute_reply.started": "2026-01-03T14:24:08.148831Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Mask2FormerLayer(nn.Module):\n",
    "    def __init__(self, dim, nheads=8):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(dim, nheads, batch_first=True)\n",
    "        self.cross_attn = nn.MultiheadAttention(dim, nheads, batch_first=True)\n",
    "\n",
    "        self.linear1 = nn.Linear(dim, dim * 4)\n",
    "        self.linear2 = nn.Linear(dim * 4, dim)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.norm3 = nn.LayerNorm(dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, queries, key_value, mask):\n",
    "        # self attention\n",
    "        q = self.norm1(queries)\n",
    "        q2, _ = self.self_attn(q, q, q)\n",
    "        q = queries + self.dropout(q2)\n",
    "\n",
    "        # masked cross attention\n",
    "        q_norm = self.norm2(q)\n",
    "\n",
    "        # flatten pixel features\n",
    "        B, C, H, W = key_value.shape\n",
    "        kv = key_value.flatten(2).transpose(1, 2)  # (B, HW, C)\n",
    "\n",
    "        # mask → attention bias\n",
    "        if mask is not None:\n",
    "            m = mask.flatten(2).permute(0, 2, 1)  # (B, HW, 1)\n",
    "            attn_mask = (m < 0.5).repeat(1, 1, q.size(1))  # False = allowed\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        q2, _ = self.cross_attn(q_norm, kv, kv,\n",
    "                                attn_mask=None)\n",
    "        q = q + self.dropout(q2)\n",
    "\n",
    "        # FFN\n",
    "        q_norm = self.norm3(q)\n",
    "        f = self.linear2(F.relu(self.linear1(q_norm)))\n",
    "        q = q + self.dropout(f)\n",
    "\n",
    "        return q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T14:24:08.314405Z",
     "iopub.status.busy": "2026-01-03T14:24:08.314134Z",
     "iopub.status.idle": "2026-01-03T14:24:08.319595Z",
     "shell.execute_reply": "2026-01-03T14:24:08.318837Z",
     "shell.execute_reply.started": "2026-01-03T14:24:08.314387Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MaskHead(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.feat_proj = nn.Conv2d(dim, dim, 1)\n",
    "        self.query_proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, pixel_features, queries):\n",
    "        B, C, H, W = pixel_features.shape\n",
    "        feat = self.feat_proj(pixel_features)\n",
    "        feat_flat = feat.view(B, C, H * W)\n",
    "\n",
    "        q = self.query_proj(queries)\n",
    "\n",
    "        logits = torch.einsum(\"bnd,bdp->bnp\", q, feat_flat)\n",
    "\n",
    "        masks = logits.view(B, -1, H, W)\n",
    "        return masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T14:24:10.219228Z",
     "iopub.status.busy": "2026-01-03T14:24:10.218466Z",
     "iopub.status.idle": "2026-01-03T14:24:10.225547Z",
     "shell.execute_reply": "2026-01-03T14:24:10.224771Z",
     "shell.execute_reply.started": "2026-01-03T14:24:10.219203Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Mask2FormerDecoder(nn.Module):\n",
    "    def __init__(self, dim, num_queries=100, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_queries = num_queries\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.query_embed = nn.Parameter(torch.randn(num_queries, dim))\n",
    "\n",
    "        self.pixel_decoder = PixelDecoder(dim)\n",
    "        self.layers = nn.ModuleList([Mask2FormerLayer(dim) for _ in range(num_layers)])\n",
    "        self.mask_head = MaskHead(dim)\n",
    "\n",
    "    def forward(self, fused_features):\n",
    "        B = fused_features.size(0)\n",
    "\n",
    "        p2, p3, p4 = self.pixel_decoder(fused_features)\n",
    "        pixel_features = p2\n",
    "        Hf, Wf = pixel_features.shape[-2:]\n",
    "\n",
    "        queries = self.query_embed.unsqueeze(0).repeat(B, 1, 1)\n",
    "\n",
    "        mask_for_next = None\n",
    "\n",
    "        for layer in self.layers:\n",
    "            queries = layer(queries, pixel_features, mask_for_next)\n",
    "            masks_q = self.mask_head(pixel_features, queries)\n",
    "            mask_for_next = masks_q.sigmoid().mean(1, keepdim=True)\n",
    "\n",
    "        # final mask = average of query masks\n",
    "        final_mask = masks_q.mean(1, keepdim=True)\n",
    "        # upsample to patch resolution (assuming 4x)\n",
    "        final_mask = F.interpolate(\n",
    "            final_mask, scale_factor=4, mode=\"bilinear\", align_corners=False\n",
    "        )\n",
    "\n",
    "        return final_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T14:24:10.390746Z",
     "iopub.status.busy": "2026-01-03T14:24:10.390531Z",
     "iopub.status.idle": "2026-01-03T14:24:10.396571Z",
     "shell.execute_reply": "2026-01-03T14:24:10.395775Z",
     "shell.execute_reply.started": "2026-01-03T14:24:10.390731Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Only last layer of features are extracted using this model\n",
    "# class SiameseDINOv3_M2F(nn.Module):\n",
    "#     def __init__(self, vit_name):\n",
    "#         super().__init__()\n",
    "#         self.backbone = ViTBackboneFeatures(vit_name)\n",
    "#         self.dim = self.backbone.out_channels\n",
    "#         self.fusion = NormFusion(self.dim)\n",
    "#         self.decoder = Mask2FormerDecoder(self.dim)\n",
    "\n",
    "#     def forward(self, pre, post):\n",
    "#         f_pre = self.backbone(pre)\n",
    "#         f_post = self.backbone(post)\n",
    "#         fused = self.fusion(f_pre, f_post)\n",
    "#         out = self.decoder(fused)\n",
    "#         return out\n",
    "\n",
    "# Mid and Last layers of features are extracted using this model\n",
    "class SiameseDINOv3_FPN(nn.Module):\n",
    "    def __init__(self, vit_name, out_indices=(4,8,12), fpn_dim=256):\n",
    "        super().__init__()\n",
    "        self.backbone = ViTBackboneMultiScale(vit_name, out_indices=out_indices)\n",
    "        self.fusions = nn.ModuleList([MultiScaleFusion(c, fpn_dim) for c in self.backbone.channels])\n",
    "        self.decoder = FPNDecoder([fpn_dim]*len(self.backbone.channels), fpn_dim=fpn_dim)\n",
    "\n",
    "    def forward(self, pre, post):\n",
    "        feats_pre  = self.backbone(pre)\n",
    "        feats_post = self.backbone(post)\n",
    "\n",
    "        fused = [fus(a,b) for fus, a, b in zip(self.fusions, feats_pre, feats_post)]\n",
    "        logits = self.decoder(fused)  # (B,1,h,w)\n",
    "\n",
    "        logits = F.interpolate(logits, size=pre.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        return logits  # (B,1,H,W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T14:24:12.792094Z",
     "iopub.status.busy": "2026-01-03T14:24:12.791395Z",
     "iopub.status.idle": "2026-01-03T14:24:12.796568Z",
     "shell.execute_reply": "2026-01-03T14:24:12.795840Z",
     "shell.execute_reply.started": "2026-01-03T14:24:12.792070Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Binary Cross-Entropy\n",
    "# def bce_loss(logits, targets, pos_weight=3.0):\n",
    "#     targets = targets.unsqueeze(1).float()\n",
    "#     pw = torch.tensor([pos_weight], device=logits.device)\n",
    "#     return F.binary_cross_entropy_with_logits(\n",
    "#         logits, targets, pos_weight=pw\n",
    "#     )\n",
    "\n",
    "# Binary Cross-Entropy + DICE\n",
    "# def dice_loss(probs, targets, eps=1e-6):\n",
    "#     targets = targets.unsqueeze(1).float()\n",
    "#     inter = (probs * targets).sum(dim=(2,3))\n",
    "#     union = probs.sum(dim=(2,3)) + targets.sum(dim=(2,3))\n",
    "#     dice = (2*inter + eps) / (union + eps)\n",
    "#     return 1 - dice.mean()\n",
    "\n",
    "# def bce_dice_loss(logits, targets, pos_weight=3.0):\n",
    "#     pw = torch.tensor([pos_weight], device=logits.device)\n",
    "#     bce = F.binary_cross_entropy_with_logits(\n",
    "#         logits, targets.unsqueeze(1).float(), pos_weight=pw\n",
    "#     )\n",
    "#     probs = torch.sigmoid(logits)\n",
    "#     d = dice_loss(probs, targets)\n",
    "#     return bce + d\n",
    "\n",
    "# Focal loss\n",
    "def focal_loss_with_logits(logits, targets, alpha=0.5, gamma=1.0):\n",
    "    targets = targets.unsqueeze(1).float()\n",
    "    bce = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\")\n",
    "    p = torch.sigmoid(logits)\n",
    "    pt = p*targets + (1-p)*(1-targets)          # prob of the true class\n",
    "    w = alpha*targets + (1-alpha)*(1-targets)   # class weight\n",
    "    loss = w * (1-pt).pow(gamma) * bce\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T14:24:12.970454Z",
     "iopub.status.busy": "2026-01-03T14:24:12.970233Z",
     "iopub.status.idle": "2026-01-03T14:24:13.777033Z",
     "shell.execute_reply": "2026-01-03T14:24:13.776473Z",
     "shell.execute_reply.started": "2026-01-03T14:24:12.970437Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 86, 82)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = OSCDDataset(CFG.root, CFG.train_cities, CFG.patch_size, CFG.stride, augment=True)\n",
    "val_ds = OSCDDataset(CFG.root, CFG.validation_cities, CFG.patch_size, CFG.stride, augment=False)\n",
    "test_ds  = OSCDDataset(CFG.root, CFG.test_cities,  CFG.patch_size, CFG.stride, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=CFG.batch_size, shuffle=True,\n",
    "                          num_workers=CFG.num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=CFG.batch_size, shuffle=False,\n",
    "                        num_workers=CFG.num_workers, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=CFG.batch_size, shuffle=False,\n",
    "                          num_workers=CFG.num_workers, pin_memory=True)\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T14:24:15.197531Z",
     "iopub.status.busy": "2026-01-03T14:24:15.197165Z",
     "iopub.status.idle": "2026-01-03T14:24:15.206942Z",
     "shell.execute_reply": "2026-01-03T14:24:15.206391Z",
     "shell.execute_reply.started": "2026-01-03T14:24:15.197508Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, threshold=0.5):\n",
    "    model.eval()\n",
    "\n",
    "    tp = fp = fn = tn = 0\n",
    "\n",
    "    for pre, post, mask in loader:\n",
    "        pre, post, mask = pre.to(device), post.to(device), mask.to(device).long()\n",
    "\n",
    "        logits = model(pre, post)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        pred = (probs > threshold).long().squeeze(1)\n",
    "\n",
    "        tp += ((pred == 1) & (mask == 1)).sum().item()\n",
    "        fp += ((pred == 1) & (mask == 0)).sum().item()\n",
    "        fn += ((pred == 0) & (mask == 1)).sum().item()\n",
    "        tn += ((pred == 0) & (mask == 0)).sum().item()\n",
    "\n",
    "    eps = 1e-6\n",
    "\n",
    "    # Metrics\n",
    "    precision = tp / (tp + fp + eps)\n",
    "    recall    = tp / (tp + fn + eps)\n",
    "    f1        = (2 * precision * recall) / (precision + recall + eps)\n",
    "    iou       = tp / (tp + fp + fn + eps)\n",
    "\n",
    "    # Overall Accuracy (OA)\n",
    "    oa = (tp + tn) / (tp + tn + fp + fn + eps)\n",
    "\n",
    "    return iou, f1, precision, recall, oa\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, threshold=0.5):\n",
    "    model.eval()\n",
    "\n",
    "    tp = fp = fn = tn = 0\n",
    "\n",
    "    for pre, post, mask in loader:\n",
    "        pre, post, mask = pre.to(device), post.to(device), mask.to(device).long()\n",
    "\n",
    "        logits = model(pre, post)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        pred = (probs > threshold).long().squeeze(1)\n",
    "\n",
    "        tp += ((pred == 1) & (mask == 1)).sum().item()\n",
    "        fp += ((pred == 1) & (mask == 0)).sum().item()\n",
    "        fn += ((pred == 0) & (mask == 1)).sum().item()\n",
    "        tn += ((pred == 0) & (mask == 0)).sum().item()\n",
    "\n",
    "    eps = 1e-6\n",
    "\n",
    "    # Metrics\n",
    "    precision = tp / (tp + fp + eps)\n",
    "    f1        = (2 * precision * (tp / (tp + fn + eps))) / \\\n",
    "                (precision + (tp / (tp + fn + eps)) + eps)\n",
    "    iou       = tp / (tp + fp + fn + eps)\n",
    "\n",
    "    # Accuracies\n",
    "    change_acc    = tp / (tp + fn + eps)   # OSCD Change accuracy\n",
    "    no_change_acc = tn / (tn + fp + eps)   # OSCD No-change accuracy\n",
    "    oa            = (tp + tn) / (tp + tn + fp + fn + eps)\n",
    "\n",
    "    return iou, f1, precision, oa, change_acc, no_change_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T14:27:07.232543Z",
     "iopub.status.busy": "2026-01-03T14:27:07.231728Z",
     "iopub.status.idle": "2026-01-03T14:46:21.309622Z",
     "shell.execute_reply": "2026-01-03T14:46:21.308650Z",
     "shell.execute_reply.started": "2026-01-03T14:27:07.232509Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = SiameseDINOv3_FPN(CFG.vit_name).to(CFG.device)\n",
    "\n",
    "def unfreeze_last_vit_blocks(model, n_blocks=2):\n",
    "    # Freeze all ViT params, then unfreeze only last n transformer blocks + final norm.\n",
    "    vit = model.backbone.backbone.model\n",
    "    blocks = vit.blocks\n",
    "\n",
    "    # freeze all vit params\n",
    "    for p in vit.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # unfreeze last n blocks\n",
    "    for blk in blocks[-n_blocks:]:\n",
    "        for p in blk.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    # unfreeze final norm\n",
    "    if hasattr(vit, \"norm\"):\n",
    "        for p in vit.norm.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "# initial freeze\n",
    "for p in model.backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": model.fusions.parameters(),  \"lr\": CFG.lr_decoder},\n",
    "    {\"params\": model.decoder.parameters(), \"lr\": CFG.lr_decoder},\n",
    "    {\"params\": filter(lambda p: p.requires_grad, model.backbone.parameters()), \"lr\": CFG.lr_backbone},\n",
    "], weight_decay=1e-4)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "    optimizer, start_factor=1.0, end_factor=0.0, total_iters=CFG.epochs\n",
    ")\n",
    "\n",
    "history = {\"loss\": [], \"iou\": [], \"f1\": [], \"prec\": [], \"oa\": [], \"chg\": [], \"no_chg\": []}\n",
    "best_iou = 0.0\n",
    "has_unfrozen = False\n",
    "\n",
    "for epoch in range(CFG.epochs):\n",
    "    model.train()\n",
    "\n",
    "    # Partial unfreeze trigger (only once)\n",
    "    if (epoch >= CFG.freeze_backbone_epochs) and (not has_unfrozen):\n",
    "        print(f\"[INFO] Unfreezing last {CFG.unfreeze_blocks} ViT blocks at epoch {epoch}\")\n",
    "        unfreeze_last_vit_blocks(model, n_blocks=CFG.unfreeze_blocks)\n",
    "        has_unfrozen = True\n",
    "\n",
    "        # rebuild optimizer so newly-unfrozen backbone params are included\n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {\"params\": model.fusions.parameters(),  \"lr\": CFG.lr_decoder},\n",
    "            {\"params\": model.decoder.parameters(), \"lr\": CFG.lr_decoder},\n",
    "            {\"params\": filter(lambda p: p.requires_grad, model.backbone.parameters()), \"lr\": CFG.lr_backbone},\n",
    "        ], weight_decay=1e-4)\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer,\n",
    "            start_factor=1.0,\n",
    "            end_factor=0.0,\n",
    "            total_iters=CFG.epochs - epoch\n",
    "        )\n",
    "\n",
    "    running = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CFG.epochs}\")\n",
    "\n",
    "    for pre, post, mask in pbar:\n",
    "        pre  = pre.to(CFG.device, non_blocking=True)\n",
    "        post = post.to(CFG.device, non_blocking=True)\n",
    "        mask = mask.to(CFG.device, non_blocking=True)\n",
    "\n",
    "        logits = model(pre, post)\n",
    "        loss = focal_loss_with_logits(logits, mask)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running += loss.item()\n",
    "        pbar.set_postfix(loss=running / (pbar.n + 1e-6))\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # ---- VALIDATION ----\n",
    "    iou, f1, precision, oa, change_acc, no_change_acc = evaluate(\n",
    "        model, val_loader, CFG.device, threshold=0.35\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"[VAL] IoU={iou:.4f} | F1={f1:.4f} | \"\n",
    "        f\"P={precision:.4f} | OA={oa:.4f} | \"\n",
    "        f\"ChangeAcc={change_acc:.4f} | NoChangeAcc={no_change_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "    # log\n",
    "    history[\"loss\"].append(running / max(1, len(train_loader)))\n",
    "    history[\"iou\"].append(iou)\n",
    "    history[\"f1\"].append(f1)\n",
    "    history[\"prec\"].append(precision)\n",
    "    history[\"oa\"].append(oa)\n",
    "    history[\"chg\"].append(change_acc)\n",
    "    history[\"no_chg\"].append(no_change_acc)\n",
    "\n",
    "    # save best\n",
    "    if iou > best_iou:\n",
    "        best_iou = iou\n",
    "        torch.save(model.state_dict(), CFG.out_dir / \"best.pt\")\n",
    "        print(\"  saved best.pt\")\n",
    "\n",
    "print(\"Training done. Best IoU:\", best_iou)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T14:47:34.164345Z",
     "iopub.status.busy": "2026-01-03T14:47:34.164049Z",
     "iopub.status.idle": "2026-01-03T14:47:36.684608Z",
     "shell.execute_reply": "2026-01-03T14:47:36.683829Z",
     "shell.execute_reply.started": "2026-01-03T14:47:34.164324Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# 1. Loss\n",
    "plt.subplot(2, 4, 1)\n",
    "plt.plot(history[\"loss\"], '-o')\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "# 2. IoU\n",
    "plt.subplot(2, 4, 2)\n",
    "plt.plot(history[\"iou\"], '-o')\n",
    "plt.title(\"Validation IoU\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "# 3. F1\n",
    "plt.subplot(2, 4, 3)\n",
    "plt.plot(history[\"f1\"], '-o')\n",
    "plt.title(\"Validation F1\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "# 4. Precision\n",
    "plt.subplot(2, 4, 4)\n",
    "plt.plot(history[\"prec\"], '-o')\n",
    "plt.title(\"Validation Precision\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "# 5. Change Accuracy (OSCD)\n",
    "plt.subplot(2, 4, 5)\n",
    "plt.plot(history[\"chg\"], '-o')\n",
    "plt.title(\"Validation Change Acc.\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "# 6. No-change Accuracy (OSCD)\n",
    "plt.subplot(2, 4, 6)\n",
    "plt.plot(history[\"no_chg\"], '-o')\n",
    "plt.title(\"Validation No-change Acc.\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "\n",
    "# 7. Overall Accuracy\n",
    "plt.subplot(2, 4, 7)\n",
    "plt.plot(history[\"oa\"], '-o')\n",
    "plt.title(\"Validation Overall Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_metrics.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T14:48:31.000830Z",
     "iopub.status.busy": "2026-01-03T14:48:30.999820Z",
     "iopub.status.idle": "2026-01-03T14:51:39.876324Z",
     "shell.execute_reply": "2026-01-03T14:51:39.875532Z",
     "shell.execute_reply.started": "2026-01-03T14:48:31.000789Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def tune_threshold(model, loader, device):\n",
    "    thresholds = np.linspace(0.2, 0.6, 41)\n",
    "    best = {\"thr\": 0.5, \"f1\": -1.0}\n",
    "\n",
    "    for thr in thresholds:\n",
    "        iou, f1, prec, oa, change_acc, no_change_acc = evaluate(\n",
    "            model, loader, device, threshold=thr\n",
    "        )\n",
    "\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best = {\n",
    "                \"thr\": float(thr),\n",
    "                \"iou\": float(iou),\n",
    "                \"f1\": float(f1),\n",
    "                \"prec\": float(prec),\n",
    "                \"oa\": float(oa),\n",
    "                \"change_acc\": float(change_acc),\n",
    "                \"no_change_acc\": float(no_change_acc),\n",
    "            }\n",
    "\n",
    "    return best\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(CFG.out_dir/\"best.pt\", map_location=CFG.device))\n",
    "best = tune_threshold(model, val_loader, CFG.device)\n",
    "\n",
    "print(\n",
    "    f\"[VAL-THR] best_thr={best['thr']:.2f} | \"\n",
    "    f\"IoU={best['iou']:.4f} | F1={best['f1']:.4f} | \"\n",
    "    f\"Prec={best['prec']:.4f} | \"\n",
    "    f\"ChangeAcc={best['change_acc']:.4f} | NoChangeAcc={best['no_change_acc']:.4f} | \"\n",
    "    f\"OA={best['oa']:.4f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T14:51:39.878478Z",
     "iopub.status.busy": "2026-01-03T14:51:39.877880Z",
     "iopub.status.idle": "2026-01-03T14:51:39.892007Z",
     "shell.execute_reply": "2026-01-03T14:51:39.891389Z",
     "shell.execute_reply.started": "2026-01-03T14:51:39.878451Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def sliding_indices(length, patch_size, stride):\n",
    "    # If image is smaller than one patch, we will just use start at 0\n",
    "    if length <= patch_size:\n",
    "        return [0]\n",
    "\n",
    "    idxs = list(range(0, length - patch_size + 1, stride))\n",
    "    last_start = length - patch_size\n",
    "    if idxs[-1] != last_start:\n",
    "        idxs.append(last_start)\n",
    "    return idxs\n",
    "\n",
    "\n",
    "\n",
    "def pad_to_patch(x, patch_size):\n",
    "    h, w, c = x.shape\n",
    "    pad_h = max(0, patch_size - h)\n",
    "    pad_w = max(0, patch_size - w)\n",
    "    if pad_h > 0 or pad_w > 0:\n",
    "        x = np.pad(x, ((0, pad_h), (0, pad_w), (0, 0)), mode=\"reflect\")\n",
    "    return x\n",
    "\n",
    "@torch.no_grad()\n",
    "def tta_predict_prob(model, pp_t, qq_t):\n",
    "    preds = []\n",
    "\n",
    "    # original\n",
    "    logits = model(pp_t, qq_t)\n",
    "    preds.append(torch.sigmoid(logits))\n",
    "\n",
    "    # horizontal flip\n",
    "    logits = model(torch.flip(pp_t, dims=[3]), torch.flip(qq_t, dims=[3]))\n",
    "    preds.append(torch.flip(torch.sigmoid(logits), dims=[3]))\n",
    "\n",
    "    # vertical flip\n",
    "    logits = model(torch.flip(pp_t, dims=[2]), torch.flip(qq_t, dims=[2]))\n",
    "    preds.append(torch.flip(torch.sigmoid(logits), dims=[2]))\n",
    "\n",
    "    # hv flip (both)\n",
    "    logits = model(torch.flip(pp_t, dims=[2,3]), torch.flip(qq_t, dims=[2,3]))\n",
    "    preds.append(torch.flip(torch.sigmoid(logits), dims=[2,3]))\n",
    "\n",
    "    prob = torch.stack(preds, dim=0).mean(dim=0)\n",
    "    return prob[0, 0].cpu().numpy()\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_city(model, city, patch_size=256, stride=128, threshold=0.5):\n",
    "    model.eval()\n",
    "\n",
    "    city_dir = CFG.root / city\n",
    "    pre  = np.array(Image.open(city_dir/\"img1.png\").convert(\"RGB\"))\n",
    "    post = np.array(Image.open(city_dir/\"img2.png\").convert(\"RGB\"))\n",
    "\n",
    "    H, W, _ = pre.shape\n",
    "    out = np.zeros((H, W), dtype=np.float32)\n",
    "    cnt = np.zeros((H, W), dtype=np.float32)\n",
    "\n",
    "    ys = sliding_indices(H, patch_size, stride)\n",
    "    xs = sliding_indices(W, patch_size, stride)\n",
    "\n",
    "    for y in ys:\n",
    "        for x in xs:\n",
    "            pp = pre[y:y+patch_size, x:x+patch_size]\n",
    "            qq = post[y:y+patch_size, x:x+patch_size]\n",
    "\n",
    "            # actual (unpadded) region size\n",
    "            h0, w0 = pp.shape[:2]\n",
    "\n",
    "            # pad to patch_size x patch_size if needed\n",
    "            pp = pad_to_patch(pp, patch_size)\n",
    "            qq = pad_to_patch(qq, patch_size)\n",
    "\n",
    "            # to tensor + normalize\n",
    "            pp_t = torch.from_numpy(pp).permute(2,0,1).float() / 255.\n",
    "            qq_t = torch.from_numpy(qq).permute(2,0,1).float() / 255.\n",
    "\n",
    "            pp_t = (pp_t - IMAGENET_MEAN) / IMAGENET_STD\n",
    "            qq_t = (qq_t - IMAGENET_MEAN) / IMAGENET_STD\n",
    "\n",
    "            pp_t = pp_t.unsqueeze(0).to(CFG.device)\n",
    "            qq_t = qq_t.unsqueeze(0).to(CFG.device)\n",
    "\n",
    "            logits = model(pp_t, qq_t)\n",
    "            prob = torch.sigmoid(logits)[0, 0].cpu().numpy()  # (patch_size, patch_size)\n",
    "            # prob = tta_predict_prob(model, pp_t, qq_t) # Make use of TTA\n",
    "\n",
    "\n",
    "            # write back ONLY the valid part (before padding)\n",
    "            out[y:y+h0, x:x+w0] += prob[:h0, :w0]\n",
    "            cnt[y:y+h0, x:x+w0] += 1.0\n",
    "\n",
    "    out /= np.maximum(cnt, 1e-6)\n",
    "    return out, (out > threshold).astype(np.uint8) * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T14:52:44.162566Z",
     "iopub.status.busy": "2026-01-03T14:52:44.161894Z",
     "iopub.status.idle": "2026-01-03T14:52:54.465574Z",
     "shell.execute_reply": "2026-01-03T14:52:54.464669Z",
     "shell.execute_reply.started": "2026-01-03T14:52:44.162535Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nTest Evaluation\")\n",
    "model.eval()\n",
    "\n",
    "best_thr = best[\"thr\"]\n",
    "\n",
    "test_iou, test_f1, test_prec, test_oa, test_change_acc, test_no_change_acc = evaluate(\n",
    "    model, test_loader, CFG.device, threshold=best_thr\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"[TEST] \"\n",
    "    f\"IoU={test_iou:.4f} | \"\n",
    "    f\"F1={test_f1:.4f} | \"\n",
    "    f\"Prec={test_prec:.4f} | \"\n",
    "    f\"ChangeAcc={test_change_acc:.4f} | \"\n",
    "    f\"NoChangeAcc={test_no_change_acc:.4f} | \"\n",
    "    f\"OA={test_oa:.4f}\"\n",
    ")\n",
    "\n",
    "print(\"\\nSaving sample Test predictions\")\n",
    "\n",
    "for city in CFG.test_cities:\n",
    "    prob, pred = predict_city(model, city, threshold=best_thr)\n",
    "    out_path = CFG.pred_dir / f\"{city}_pred.png\"\n",
    "    Image.fromarray(pred).save(out_path)\n",
    "    print(f\"Saved: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: runs\\predictions\\brasilia_pred.png\n",
      "Saved: runs\\predictions\\montpellier_pred.png\n",
      "Saved: runs\\predictions\\norcia_pred.png\n",
      "Saved: runs\\predictions\\rio_pred.png\n",
      "Saved: runs\\predictions\\saclay_w_pred.png\n",
      "Saved: runs\\predictions\\valencia_pred.png\n",
      "Saved: runs\\predictions\\dubai_pred.png\n",
      "Saved: runs\\predictions\\lasvegas_pred.png\n",
      "Saved: runs\\predictions\\milano_pred.png\n",
      "Saved: runs\\predictions\\chongqing_pred.png\n"
     ]
    }
   ],
   "source": [
    "model = SiameseDINOv3_FPN(CFG.vit_name).to(CFG.device)\n",
    "state = torch.load(CFG.path_ckpt, map_location=CFG.device)\n",
    "\n",
    "best_thr = 0.34\n",
    "\n",
    "if isinstance(state, dict) and \"model\" in state:\n",
    "    state = state[\"model\"]\n",
    "\n",
    "model.load_state_dict(state, strict=True)\n",
    "model.to(CFG.device)\n",
    "model.eval()\n",
    "\n",
    "for city in CFG.test_cities:\n",
    "    prob, pred = predict_city(model, city, threshold=best_thr)\n",
    "    out_path = CFG.pred_dir / f\"{city}_pred.png\"\n",
    "    Image.fromarray(pred).save(out_path)\n",
    "    print(f\"Saved: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8927407,
     "sourceId": 14013488,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
